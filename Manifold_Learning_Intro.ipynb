{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold Learning: Introduction and Foundational Algorithms\n",
    "### *Mathematical Theory with Examples and Applications in Python*\n",
    "`Drew Wilimitis`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "![KleinDual](https://user-images.githubusercontent.com/16658498/55211718-3e737900-51bb-11e9-8a4a-37ecf83c9ab0.gif)\n",
    "<font size = 2> Image source: Klein's quartic curve by Greg Egan https://www.gregegan.net/SCIENCE/KleinQuartic/KleinQuartic.html <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "In this series of Jupyter Notebooks, I present the mathematical details of some foundational manifold learning algorithms alongside some applied examples using the `sklearn.manifold` library. I begin with this introduction and provide a very brief, basic overview of some relevant topology and differential geometry, as well as the idea of topological data analysis. I then discuss machine learning with high dimensional datasets and the manifold hypothesis.   \n",
    "<br>\n",
    "This project can be viewed with the following links to jupyter notebook viewer. The original jupyter notebook files used to create this project can be downloaded here: https://github.com/drewwilimitis/Manifold-Learning \n",
    "\n",
    "\n",
    "Contents\n",
    "------\n",
    "\n",
    "* [**Introduction:**](https://nbviewer.jupyter.org/github/drewwilimitis/Manifold-Learning/blob/master/Manifold_Learning_Intro.ipynb)\n",
    "    - Overview of manifolds and the basic topology of data\n",
    "    - Statistical learning and instrinsic dimensionality\n",
    "    - The manifold hypothesis \n",
    "\n",
    "* [**Chapter 1: Multidimensional Scaling**](https://nbviewer.jupyter.org/github/drewwilimitis/Manifold-Learning/blob/master/Multidimensional_Scaling.ipynb)\n",
    "    - Classical, metric, and non-metric MDS algorithms\n",
    "    - Example applications to quantitative psychology and social science\n",
    "    \n",
    "* [**Chapter 2: ISOMAP**](https://nbviewer.jupyter.org/github/drewwilimitis/Manifold-Learning/blob/master/Isomap.ipynb)\n",
    "    - Geodesic distances and the isometric mapping algorithm\n",
    "    - Implementation details and application \n",
    "\n",
    "* [**Chapter 3: Local Linear Embedding**](https://nbviewer.jupyter.org/github/drewwilimitis/Manifold-Learning/blob/master/Locally_Linear_Embedding.ipynb)\n",
    "    - Locally linear reconstructions and optimization problems\n",
    "    - Example applications with image data\n",
    "    \n",
    "* [**Chapter 4: Laplacian Eigenmaps/Spectral Embedding**](https://nbviewer.jupyter.org/github/drewwilimitis/Manifold-Learning/blob/master/Laplacian-Eigenmaps.ipynb)\n",
    "    - From the general to the discrete Laplacian operators\n",
    "    - Visualizing spectral embedding with the networkx library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: Topology, Metric Spaces, and Manifolds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of the theoretical foundation for applied dimensionality reduction, we start very generally with topology and the notion of a *mathematical space*.\n",
    "\n",
    "A **topological space** is defined only in terms of set theory, and therefore provides a very general way to define a *mathematical space* - which is really just a set with some additional structure. It's not especially necessary to fully understand the following formal definition, but it can certainly be appreciated for its abstraction. A **topological space** is defined as a set $X$ with a collection of open neighborhoods, or subsets, $T$ that follow a few basic axioms: <br>\n",
    "1. The (trivial) subsets $X$ and the empty set $\\emptyset$ are in $T$\n",
    "2. Whenever sets $A$ and $B$ are in $T,$ then so is $A \\cap B$ .\n",
    "3. Whenever two or more sets are in $T$ , then so is their union\n",
    "\n",
    "From this definition of a topological space, we add additional structure, such as a metric, that allows us to characterize the relationships between elements of the set. Since notions of similarity and distance, which we formalize as metrics, are central to geometry, machine learning tasks, and especially manifold learning algorithms, we formally define a **metric space**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **metric space** is a pair $(M, d)$ where $M$ is a set and $d$ is a metric on $M$, where a metric is a function $d : M \\times M \\rightarrow \\mathbb{R}$ such that for any $x, y, z \\in M$ we have:<br>\n",
    "<br>\n",
    "$$\n",
    "\\begin{array}{ll}{d(x, y)=0 \\Leftrightarrow x=y} \\ \\ \\ {\\text { identity }} \\\\ {d(x, y)=d(y, x)} \\ \\ \\ {\\text { symmetry }} \\\\ {d(x, z) \\leq d(x, y)+d(y, z) \\ \\ \\ \\text { triangle inequality }}\\end{array}\n",
    "$$\n",
    "<br>\n",
    "The metric function has these rules that we would expect for distances. Just like we can't have negative distance, we also have $d(x, y) \\geq 0$:<br>\n",
    "<br>\n",
    "$$\n",
    "\\begin{array}{ll}{d(x, y)+d(y, x) \\geq d(x, x)} & {\\text { by triangle inequality }} \\\\ {d(x, y)+d(x, y) \\geq d(x, x)} & {\\text { by symmetry }} \\\\ {2 d(x, y) \\geq 0} & {\\text { by identity  }} \\\\ {d(x, y) \\geq 0} & {\\text { non-negativity }}\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://i.stack.imgur.com/CWRld.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure shows open unit neighborhoods or 'unit balls' centered at the origin for different metric spaces, where the neighborhoods are defined as the set of elements within a given radius $\\varepsilon$: <br>\n",
    "<br>\n",
    "$$B_{\\varepsilon}(x) :=\\{y \\in M | d(x, y)<\\varepsilon\\}$$ <br>\n",
    "For the unit neighborhood we let $\\varepsilon = 1$. From left to right we have the metric spaces $(\\mathbb{R^2}, d)$ with the following metrics: <br>\n",
    "<br>\n",
    "$$\n",
    "\\begin{array}{ll}{d_2(x, y)=\\sqrt{\\left(x_{1}-y_{1}\\right)^{2}+\\left(x_{2}-y_{2}\\right)^{2}}} & {\\text { the Euclidean metric or } L_2 \\text{norm}} \\\\ {d_{\\infty}(x, y)=\\max \\left(\\left|x_{1}-y_{1}\\right|,\\left|x_{2}-y_{2}\\right|\\right)} & {\\text { maximum metric or } L_{\\infty} \\text{norm} } \\\\ {d_1(x, y)=\\left|x_{1}-y_{1}\\right|+\\left|x_{2}-y_{2}\\right|} & {\\text { taxicab metric or} \\ L_{1} \\text{norm}  } \\end{array}\n",
    "$$\n",
    "<br>\n",
    "Although the Euclidean metric is often standard, there are many cases in machine learning where the specification of the metric is crucially important. Specifying the $L_1$ or $L_2$ norm defines regularization techniques like LASSO and Elastic Net for penalized regression. Many methods in dimensionality reduction and unsupervised learning consider local neighborhoods, which are dependent on the choice of metric, and this is often critical to learning geometric and topological properties.\n",
    "\n",
    "With the definition of metric spaces, we can continue to build up from general topological spaces and begin to define concepts that are based on distances, like limits, continuous functions, and convergence, eventually getting into more familiar notions like derivatives and integrals from the study of calculus. <br>\n",
    "\n",
    "Just like metric spaces are just topological spaces with this additional metric structure, a **manifold** is also just a topological space with the additional property that for each point, the surrounding neighborhood locally resembles Euclidean space (which we can take to be the reals $\\mathbb{R}$ with the standard Euclidean distance metric). For this reason, manifolds (specifically a special class of manifolds called differential manifolds) allow you to use calculus on higher dimensional structures that might be globally non-Euclidean. \n",
    "\n",
    "More formally, each point of an n-dimensional **manifold** has an open neighborhood that is *homeomorphic* to an open neighborhood of Euclidean space with dimension n. Before going further we must define this new term that formalizes the notion of \"locally resembling Euclidean space\". <br>\n",
    "<br>\n",
    "A **homeomorphism** is a function $f : X \\rightarrow Y$ between two topological spaces where: <br>\n",
    "<br>\n",
    "$$\n",
    "\\begin{array}{l}{\\cdot f \\text { is a bijection (one-to-one and onto) }} \\\\ {\\cdot f \\text { is continuous }} \\\\ {\\cdot \\text { the inverse function } f^{-1} \\text { is continuous } }\\end{array}\n",
    "$$\n",
    "\n",
    "A homeomorphism is a continuous bijection (where the inverse is also continuous). We say the two topological spaces are *homeomorphic* if there exists a *homeomorphism* between them. <br>\n",
    "<br>\n",
    "**Note**: an n-dimensional manifold is only *locally* homeomorphic to Euclidean space, and in general a manifold is not necessarily globally homeomorphic to Euclidean space. <br>\n",
    "<br>\n",
    "**Note**: a manifold can also have additional properties like a metric and differentiability.\n",
    "\n",
    "Two spaces that are homeomorphic have the same topological properties like compactness and connectedness (the word homeomorphic means \"same shape\"). Although they are not precisely the same, a homeomorphism can be thought of in a similar way as a continuous deformation - the continuous stretching and bending that preserves topological equivalence. One can often imagine stretching or flattening out local neighborhoods of a manifold to get a flat Euclidean plane. <br>\n",
    "<br>\n",
    "Since a homeomorphism is a continuous bijection with a continuous inverse, we can jump back and forth between these topological spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://upload.wikimedia.org/wikipedia/commons/6/64/Circle_with_overlapping_manifold_charts.svg)<br>\n",
    "This is a simple demonstration of a manifold (the unit circle in $\\mathbb{R}^2$) with four homeomorphic transition maps. For example, the top, yellow part of the circle is mapped continuously into Euclidean space by mapping each point to its x-coordinate: $\\phi_{\\operatorname{top}}(x, y)=x$, which maps the top part of the circle into the interval (-1,1). The full demonstration with explicitly defined transition maps can be found at the figure source: https://en.wikipedia.org/wiki/Manifold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1D Manifolds - Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An n-dimensional manifold in a larger m-dimensional space ($n < m$) locally resembles an n-dimensional Euclidean hyperplane. Many simple curves in $\\mathbb{R^2}$ are locally homeomorphic to 1-dimensional Euclidean space, and we can imagine some straightforward examples of these 1D manifolds.\n",
    "\n",
    "The unit circle in $\\mathbb{R^2}$ is a 1-dimensional manifold, but a 'figure eight' curve in $\\mathbb{R^2}$ is not a 1-dimensional manifold since it is not locally homeomorphic to Euclidean space in the regions with the center crossing point. We have more examples from conics sections: circles, parabolas, hyperbolas, and ellipses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Conics_and_cubic.svg/220px-Conics_and_cubic.svg.png) <br>\n",
    "Figure source: https://en.wikipedia.org/wiki/Manifold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 1D manifolds don't have to be connected, like the manifold defined as the union of the two circles where the local homeomorphism criteria still holds. Manifolds also don't have to be finite, like the hyperbolas that extend out infinitely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2D Manifolds - Surfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://user-images.githubusercontent.com/16658498/56078883-99f45800-5db2-11e9-8adb-84f4a5c98aa7.PNG)\n",
    "Figure source: Renze, John; Rowland, Todd; and Weisstein, Eric W. \"Compact Manifold.\" From MathWorld--A Wolfram Web Resource. http://mathworld.wolfram.com/CompactManifold.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many familiar closed surfaces like the sphere, torus, and Klein bottle in $\\mathbb{R^3}$ are locally homeomorphic to 2-dimensional Euclidean space, and so these are all 2-dimensional manifolds. You can imagine a tangent plane approximating the local neighborhoods at each point on these surfaces, or flattening out a small patch of the sphere's surface so that it lies along the Euclidean plane in $\\mathbb{R^2}$. \n",
    "\n",
    "Globally, however, these surfaces are generally not going to be homeomorphic to $\\mathbb{R^2}$. You can't find a homeomorphism between the sphere and the Euclidean plane. Intuitively, you can't twist or stretch the sphere continuously to lie flat along the plane (without squishing it to a single point or folding it on top of itself - which are not bijective transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### n-Manifolds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Euclidean Space**: $\\mathbb{E}^n$ is an n-dimensional manifold, since it is homeomorphic to itself<br>\n",
    "<br>\n",
    "- **n-Sphere**: $S^{n}=\\left\\{x \\in \\mathbb{R}^{n+1} :\\|x\\|=r\\right\\}$ is an n-dimensional manifold<br>\n",
    "<br>\n",
    "- **n-Torus**: The n-Torus is an n-dimensional manifold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://user-images.githubusercontent.com/16658498/59167559-60607480-8af7-11e9-9b61-4dc4abe13b80.png)<br>\n",
    "The continuous surface of a 3-D bunny (which is modeled here by a discrete mesh) can also be defined as a manifold, and calculus can be applied to this non-Euclidean shape. Source: http://graphics.stanford.edu/data/3Dscanrep/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Big Idea of Manifolds</b> <br>\n",
    "    <br>\n",
    "    Since a manifold structure is defined by this property of *locally* resembling Euclidean space, we do not have to consider the geometric relationships of any global, extrinsically defined coordinate system and instead we can just consider the manifold's intrinsic geometry and topological properties.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Idea Behind Topological Data Analysis\n",
    "\n",
    "Geometric and topological relationships are fundamental to essentially every data analysis and machine learning task, as we use geometry to identify similarities and distinctive characteristics in the data. In classification, for example, data points that are similar (close to each other) are assigned to the same classes and data points that are significantly different (i.e. far apart in one or many of the feature dimensions) are assigned to different labels. <br>\n",
    "<br>\n",
    "We usually consider data as a finite set $X=\\left\\{x_{1}, x_{2}, \\cdots, x_{N}\\right\\} \\subset \\mathbb{R}^{D}$ sometimes called a point cloud. However, geometric and topological structures like metric spaces and manifolds are continuous, not discrete. To discover any geometric or topological properties of the data, we \\textbf{fit a continuous shape to the data}, and we must make certain assumptions about the underlying mathematical space we're working in. It is often simply assumed that our data lies in the standard Euclidean space with the typical Euclidean metric, and the data is analyzed by referencing a global, external coordinate system. However, many interesting and important structures that arise are actually non-Euclidean, and by fitting a continuous shape (such as a manifold) to the data, we can translate our data analysis task from an external, global coordinate system (possibly having very high dimensionality) into the intrinsic coordinate system defined by the assumed manifold structure itself. Ideally, the underlying manifold will capture the latent structure in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-1023b59114ce>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-1023b59114ce>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    Figure source: https://www.nature.com/articles/srep01236#author-information\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# ![title](https://raw.githubusercontent.com/drewwilimitis/Manifold-Learning/master/images/topological-data-analysis-15-638.jpg) <br>\n",
    "Figure source: https://www.nature.com/articles/srep01236#author-information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Once we assume that the data can be approximated by a manifold, then we can use the fact that topological properties remain constant with continuous deformations. We can often apply a dimensionality reduction transformation to map the data into lower dimensional spaces, while preserving important topological properties (such as connectedness and compactness). For this geometric and topological information to be useful, the assumed, underlying continuous shape needs to reveal important features of the data and the noise of the data samples needs to be relatively low.<br>\n",
    "<br>\n",
    "To further understand the utility of topological approaches and how they can uncover highly complex and informative features (to be used for visualization, dimensionality reduction, and/or as engineered features to be input to a supervised learning model) we must first consider the challenges of analyzing high dimensional data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Curse (and Blessing) of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **The Curse of Dimensionality** refers to the various challenges (such as computational complexity) that arise when applying data analysis and machine learning methods to high dimensional datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the dimensionality of a dataset increases, the volume of the high dimensional space grows exponentially. The available data can become very sparse in relation to higher dimensional volumes, where essentially all the data points become further and further apart. This makes machine learning tasks that rely on identifying feature interactions and similarities in groups of data, like clustering, extremely hard and often computationally intractable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Example - Sampling uniformly within n-dimensional unit spheres </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import some helpful libraries and configure some output settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T23:32:15.092256Z",
     "start_time": "2019-07-02T23:32:15.078296Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key text.latex.unicode in file /home/deniz/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 112 ('text.latex.unicode : False # use \"ucs\" and \"inputenc\" LaTeX packages for handling')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.4/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key savefig.frameon in file /home/deniz/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 423 ('savefig.frameon : True')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.4/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key pgf.debug in file /home/deniz/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 444 ('pgf.debug           : False')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.4/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.level in file /home/deniz/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 475 ('verbose.level  : silent      # one of silent, helpful, debug, debug-annoying')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.4/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.fileo in file /home/deniz/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 476 ('verbose.fileo  : sys.stdout  # a log filename, sys.stdout or sys.stderr')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.4/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "In /home/deniz/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/deniz/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/deniz/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/deniz/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/deniz/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/deniz/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/deniz/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/deniz/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import math\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.core.pylabtools import figsize\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "figsize(12, 6)\n",
    "\n",
    "# display multiple outputs within a cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\";\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider data sampled uniformly within the unit sphere of dimension $n$: <br> \n",
    "<br>\n",
    "$$\\mathbf{B}^{n}=\\left\\{\\left(x_{1}, x_{2}, \\ldots, x_{N}\\right) \\in \\mathbb{R}^{n} | x_{1}^{2}+x_{2}^{2}+\\cdots+x_{N}^{2}<1\\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we sample uniformly within the 2 and 3-dimensional unit spheres. We consider neighborhoods centered at the origin and the proportion of samples that lie within the neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T23:32:17.620228Z",
     "start_time": "2019-07-02T23:32:17.012885Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot unit circle in R^2\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "circ = plt.Circle((0, 0), radius=1, edgecolor='black', facecolor='None', linewidth=3, alpha=0.5)\n",
    "ax.add_patch(circ)\n",
    "circ = plt.Circle((0, 0), radius=0.5, edgecolor='black', facecolor='None', linewidth=3, linestyle='--')\n",
    "ax.add_patch(circ)\n",
    "\n",
    "# sample within unit sphere in R^2\n",
    "n = 1000\n",
    "theta = np.random.uniform(0, 2*math.pi, n)\n",
    "u = np.random.uniform(0, 1, n)\n",
    "r = np.sqrt(u)\n",
    "x = r * np.cos(theta)\n",
    "y = r * np.sin(theta)\n",
    "ax.scatter(x, y, s=10, alpha=1)\n",
    "\n",
    "# proportion within neighborhood\n",
    "p = np.sum((x**2 + y**2) <= 0.5**2) / n\n",
    "ax.scatter(0, 0, s=20, c='black')\n",
    "plt.title(\"% in neighborhood = \" + str(p))\n",
    "\n",
    "# plot unit sphere in R^3\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax.set_aspect(\"auto\")\n",
    "u, v = np.mgrid[0:2*np.pi:20j, 0:np.pi:10j]\n",
    "x = np.cos(u)*np.sin(v)\n",
    "y = np.sin(u)*np.sin(v)\n",
    "z = np.cos(v)\n",
    "ax.plot_wireframe(x, y, z, color=\"black\", alpha=0.5)\n",
    "ax.plot_wireframe(0.5*x, 0.5*y, 0.5*z, color=\"black\")\n",
    "\n",
    "# uniform sampling\n",
    "phi = np.random.uniform(0, 2*math.pi, n)\n",
    "costheta = np.random.uniform(-1, 1, n)\n",
    "u = np.random.uniform(0, 1, n)\n",
    "\n",
    "theta = np.arccos(costheta)\n",
    "r = u**(1/3)\n",
    "x = r * np.sin(theta) * np.cos(phi)\n",
    "y = r * np.sin(theta) * np.sin(phi)\n",
    "z = r * np.cos(theta)\n",
    "ax.scatter(x, y, z, alpha=0.4, s=10)\n",
    "\n",
    "# proportion within neighborhood\n",
    "p = np.sum((x**2 + y**2 + z**2) <= 0.5**2) / n\n",
    "ax.scatter([0], [0], [0], color=\"black\", s=100)\n",
    "plt.title(\"% in neighborhood = \" + str(p));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In $\\mathbb{R^2}$ the unit sphere has volume $V = \\pi \\cdot r^2$\n",
    "\n",
    "If we let p = % of samples within the neighborhood, we have p approximately equal to the volume of the neighborhood divided by the volume of the unit sphere. For $r = \\frac{1}{2}$ we have $p \\approx (\\frac{1}{2})^2 = \\frac{1}{4}$\n",
    "\n",
    "In $\\mathbb{R^3}$ the unit sphere has volume $V = \\frac{4}{3} \\pi \\cdot r^3$ and so for $r = \\frac{1}{2}$ we have $p = (\\frac{1}{2})^3 \\approx \\frac{1}{8}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In $\\mathbb{R^n}$, the unit sphere has volume that increases exponentially with a given fixed radius:\n",
    "$$\n",
    "V_{n}(R)=\\frac{\\pi^{\\frac{n}{2}}}{\\Gamma\\left(\\frac{n}{2}+1\\right)} R^{n}\n",
    "$$\n",
    "\n",
    "where in the denominator we have the Gamma function: <br>\n",
    "$$\\Gamma(n)=(n-1) ! \\text { if } n \\text { is a positive integer }$$\n",
    "\n",
    "In 10 dimensions we have the volume $$\n",
    "V = \\frac{\\pi^{5}}{120} R^{10} \\approx 2.550 \\times R^{10}\n",
    "$$\n",
    "\n",
    "and so if the radius is 1/2, the proportion of samples within our neighborhood $p \\approx 2.43^{-3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have the neighborhood contain approximately 1/4 of the samples like the sphere in $\\mathbb{R^2}$ we need to have $r \\approx 0.8$ which is nearly the size of the whole dataset. Clustering tasks become nearly impossible when the data points are so far away and near the boundary, and the volume of the high dimensional space becomes so enormous. We would need training samples that grow exponentially with the dimensionality, which quickly becomes computationally intractable.\n",
    "\n",
    "There are many methods in machine learning that attempt to reduce the dimensionality of high dimensional datasets. In some sense, they all rely on exploiting the intrinsic structure of the data, where the dataset lies closely to a lower dimensional subspace embedded within the higher dimensional space. Empirically, high dimensional data sets are not sampled uniformly, and so there are special structures and irregularities like many data points being very close together in one of the dimensions. \n",
    "\n",
    "This fact that higher dimensional datasets often have some intrinsic lower dimensionality is what I refer to as this blessing of dimensionality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'> Example - MNIST Digits </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many datasets are actually very very far from being sampled uniformly over some domain. Consider a classification problem with the MNIST handwritten digits dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T23:32:25.162830Z",
     "start_time": "2019-07-02T23:32:25.088033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784')\n",
    "mnist.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 70000 handwritten digits 0-9, where each observation is an image with $28 \\times 28 = 784$ features that can take values representing pixel intensities from 0 to 255. Here is a plot of some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T23:32:28.133942Z",
     "start_time": "2019-07-02T23:32:26.772608Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1b4a4e1d4d58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplot_kw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mjj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1250\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(6, 8, subplot_kw=dict(xticks=[], yticks=[]))\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    jj = axi.imshow(mnist.data.iloc[1250 * i].values.reshape(28, 28), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you imagine generating random images by sampling uniformly over this 784 dimensional space, the probability that you would generate an image resembling a handwritten digit is unbelievably small. There's some intrinsic structure to this dataset that could allow for dimensionality reduction, such as all the digits being centered and the corners being almost all white, common line segments and loops, and a bimodal distribution of pixel values where they are usually either black or white.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T23:32:33.124499Z",
     "start_time": "2019-07-02T23:32:32.049374Z"
    }
   },
   "outputs": [],
   "source": [
    "# explained variance from principal components\n",
    "from sklearn.decomposition import PCA\n",
    "# take subset of mnist data (1/10 of observations)\n",
    "data = mnist.data[::10]\n",
    "model = PCA()\n",
    "proj = model.fit_transform(data)\n",
    "cumsum = np.cumsum(model.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot((0, 800), (0.95, 0.95), 'r--')\n",
    "plt.plot(cumsum)\n",
    "plt.xlim(0,200)\n",
    "plt.style.use('seaborn')\n",
    "plt.title('PCA Explained Variance')\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.show()\n",
    "print('95% Explained Variance with ' + str(d) + ' Principal Components');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data is 28 x 28 = 784 dimensions, but with only the first ~150 principal component features we can preserve 95% of the variation among the handwritten digits data. Therefore, it's possible to use the principal components and project the data into this 150 dimensional space without too much information loss, as the remaining 634 dimensions are relatively uninformative. In this sense we can consider the intrinsic dimensionality of the mnist data as much lower than the extrinsic 784 dimensional space.\n",
    "\n",
    "If we have to predict the target class of a new validation sample, we know a lot already about the 784 feature values simply because we know we're predicting handwritten digits in this problem. This is the case with many other high dimensional datasets as well. For example, in an image classification task for a dataset with images of animals, samples will have discernible objects and probably some common structures like eyes, ears, tails, etc.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The Manifold Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since many real datasets lie close to some lower dimensional subspace, many techniques for dimensionality reduction are also based on assuming a lower dimensional manifold representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The **manifold hypothesis**, or **manifold assumption**, is that many real-world datasets can be approximately represented as lower dimensional manifolds that are embedded in a higher dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manifold learning** is a class of methods developed to learn this lower dimensional representation, sometimes referred to as the intrinsic dimensionality, of the data. <br>\n",
    "<br>\n",
    "In manifold learning, the data points $x_1, x_2, ..., x_N$ are assumed to be sampled from the (often uniform) distribution defined by the underlying d-dimensional manifold $M \\subset \\mathbb{R}^{D}$. We then attempt to learn the intrinsic structure of this underlying manifold. This can all be quite abstract, but below is a more concrete example with the canonical manifold learning \"swiss roll\" dataset, a 3-dimensional dataset that lies close to a nonlinear, 2-dimensional manifold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T23:32:50.227237Z",
     "start_time": "2019-07-02T23:32:48.674737Z"
    }
   },
   "outputs": [],
   "source": [
    "# draw samples to create the grid\n",
    "t = np.linspace(0, 1, 50)\n",
    "u = np.linspace(0, 1, 50)\n",
    "v = 3*np.pi/2*(.1 + 2*t)\n",
    "u,v = np.meshgrid(u,v)\n",
    "\n",
    "# swiss roll transformation\n",
    "x = -v*np.cos(v)\n",
    "y = u\n",
    "z = v*np.sin(v)\n",
    "\n",
    "# set up a figure twice as wide as it is tall\n",
    "fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "# plot 3d surface\n",
    "colors = plt.cm.jet((x**2 + z**2) / 100)\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax.plot_surface(x, y, z, rstride=1, cstride=1, facecolors=colors, cmap=plt.cm.coolwarm, linewidth=1.4, alpha=0.8)\n",
    "ax.set_title('Continuous manifold in $\\mathbb{R}^3$', size=16);\n",
    "\n",
    "# draw uniform samples from the continuous manifold\n",
    "n = 4000\n",
    "t = np.random.rand(n, 1)\n",
    "u = np.random.rand(n, 1)\n",
    "v = 3*np.pi/2*(.1 + 2*t)\n",
    "\n",
    "x = -v*np.cos(v)\n",
    "y = u\n",
    "z = v*np.sin(v)\n",
    "color = (x**2 + z**2) / 100\n",
    "color = color.reshape(n,)\n",
    "\n",
    "# set up the axes for the first plot\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax.scatter(x, y, z, cmap=plt.cm.jet, c=color);\n",
    "ax.set_title('Finite Data Samples', size=16);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical considerations must be taken into account when we try to uncover the true latent structure of our dataset. When we assume the data lies close to this embedded manifold, we view the collection of data points we have to be a finite sample drawn from the continuous distribution of the manifold. <br>\n",
    "<br>\n",
    "Therefore, statistical methods can be applied to estimate the true topological features of the manifold based on our observed, finite data samples. The bias and variance can be assessed and it's possible to determine whether the noise in our dataset allows us to derive statistically significant claims about the assumed manifold geometry.   \n",
    "\n",
    "Two related problems that arise are significant levels of noise in the input data and insufficient sampling that under-represents important regions. Consider how introducing a little noise seems to obfuscate the latent manifold geometry (this time using the `sklearn.manifold` library to generate the swiss roll)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T23:32:56.917224Z",
     "start_time": "2019-07-02T23:32:56.564823Z"
    }
   },
   "outputs": [],
   "source": [
    "# set up a figure twice as wide as it is tall\n",
    "fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "\n",
    "# sample from swiss roll dataset\n",
    "from sklearn.datasets.samples_generator import make_swiss_roll\n",
    "X, color = make_swiss_roll(n_samples = 1000, noise=1.5)\n",
    "ax = fig.add_subplot(1, 2, 1, projection = '3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], -X[:, 2], c=color, cmap=plt.cm.jet)\n",
    "ax.set_title('Sampling with Gaussian noise', size=16)\n",
    "\n",
    "# undersample from manifold\n",
    "X, color = make_swiss_roll(n_samples = 100, random_state=8888)\n",
    "ax = fig.add_subplot(1, 2, 2, projection = '3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.jet)\n",
    "ax.set_title('Undersampling', size=16);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manifold learning is intriguing and insightful in its own right and it offers a valuable way to understand and visualize latent geometries and invariant structural relationships. However, an implicit assumption in most of these methods is that they will assist supervised learning problems. Manifold learning is often incorporated in supervised learning problems as an exploratory analysis/visualization, feature extraction method, and for non-linear dimensionality reduction.<br>\n",
    "<br>\n",
    "In this project, I don't go over the details of Principal Component Analysis (PCA), except for a few cases where PCA naturally arises as a point of comparison to the other algorithms. There are many many great resources on PCA, and including an in-depth treatment of PCA would significantly expand the scope of this project. None of the material is dependent on PCA, but there is one crucial thing to remember about PCA in the broader context of manifold learning and dimensionality reduction.<br>\n",
    "<br>\n",
    "PCA involves a *linear* projection of the original data onto a selected number of principal component vectors (vectors that maximize the variance in the data). Therefore, standard methods from PCA are unable to learn *non-linear* relationships, whereas the class of algorithms in manifold learning are able to learn non-linear relationships and preserve these non-linear relationships when transforming the data to lower dimensions. <br>\n",
    "<br>\n",
    "It can be hard to know whether non-linear dimensionality reduction will simplify a machine learning task, and this isn't always the case, but often times classification and regression problems are easier after applying lower dimensional embeddings. Below is an example of manifold learning simplifying a basic classification problem, while PCA fails to learn the non-linear structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T23:33:00.417357Z",
     "start_time": "2019-07-02T23:32:59.603926Z"
    }
   },
   "outputs": [],
   "source": [
    "X, color = make_swiss_roll(n_samples = 3000)\n",
    "plt.style.use('classic')\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "fig = plt.figure(figsize = (12, 6))\n",
    "ax = fig.add_subplot(1, 2, 1, projection = '3d')\n",
    "ax.scatter(X[(color < 10), 0], X[(color < 10), 1], X[(color < 10), 2], c='blue')\n",
    "ax.scatter(X[(color >= 10), 0], X[(color >= 10), 1], X[(color >= 10), 2], c='red')\n",
    "ax.set_title('Swiss Roll', size=16)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2, projection = '3d')\n",
    "ax.scatter(X[(color < 10), 0], X[(color < 10), 1], X[(color < 10), 2], c='blue')\n",
    "ax.scatter(X[(color >= 10), 0], X[(color >= 10), 1], X[(color >= 10), 2], c='red')\n",
    "ax.set_title('Alternate View', size=16)\n",
    "ax.view_init(4, -80);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in $\\mathbb{R^3}$ it would be hard to fit a linear decision boundary and classify data points. After applying the Isomap manifold learning algorithm (covered in Part 2 of this manifold learning series) to reduce the dimensionality to $\\mathbb{R^2}$ and visualize the intrinsic dimensionality, the classification task becomes much easier. The Isomap algorithm is able to learn the shape of the data, while PCA fails, and the geometry provides the key insight into the similarities between data points.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T23:33:08.034327Z",
     "start_time": "2019-07-02T23:33:03.483490Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "data = X\n",
    "k = 8\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(data)\n",
    "X_pca = pca.transform(data)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.scatter(X_pca[(color < 10), 0], X_pca[(color < 10), 1], c='blue')\n",
    "ax.scatter(X_pca[(color >= 10), 0], X_pca[(color >= 10), 1], c='red')\n",
    "ax.set_title('PCA with two components', size=14)\n",
    "\n",
    "model = Isomap(n_components=2, n_neighbors=k)\n",
    "X_iso = model.fit_transform(data)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.scatter(X_iso[(color < 10), 0], X_iso[(color < 10), 1], c='blue')\n",
    "ax.scatter(X_iso[(color >= 10), 0], X_iso[(color >= 10), 1], c='red')\n",
    "ax.set_title('Isomap with k = ' + str(k), size=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the nonlinear Isomap transformation, we can easily fit a linear decision boundary to separate the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T23:33:14.222626Z",
     "start_time": "2019-07-02T23:33:13.782744Z"
    }
   },
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "plt.style.use('default')\n",
    "y = np.zeros((len(X_iso)))\n",
    "y[color < 10] = 0\n",
    "y[color >= 10] = 1\n",
    "y = y.astype('int')\n",
    "svm_clf = clf.fit(X_iso, y)\n",
    "plt.figure(figsize=(10,5.5))\n",
    "ax = plot_decision_regions(X_iso, y, clf=svm_clf, legend=2, markers='oo', colors='blue,red')\n",
    "# Shrink current axis by 20%\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "\n",
    "# Put a legend to the right of the current axis\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), shadow=True)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non-linear dimensionality reduction has greatly simplified the classification task, and this example demonstrates a simpler case when manifold learning can be used as a pre-processing step to improve supervised learning models. <br>\n",
    "<br>\n",
    "There are a few broad classes of these manifold learning algorithms, and the recurring themes provide an excellent entry point to explore some deeper mathematical topics like topological methods in machine learning and data analysis, which have a wide variety of applications, many of them probably yet to be discovered. There are well-developed implementations of many of these algorithms and some existing demonstrations of their effectiveness on a variety of interesting problems. Throughout this project, I go over some of these non-linear dimensionality reduction algorithms and manifold learning methods in more detail. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:19:06.422241Z",
     "start_time": "2019-07-02T22:19:06.320510Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "def css_styling():\n",
    "    styles = open(\"custom_style.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### References\n",
    "\n",
    "\n",
    "-  [1] https://en.wikipedia.org/wiki/Manifold\n",
    "-  [2] https://en.wikipedia.org/wiki/Metric_space\n",
    "-  [3] https://en.wikipedia.org/wiki/Topological_space"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('spinningup': conda)",
   "language": "python",
   "name": "python361064bitspinningupcondaa59e53df4a214fb590727ee6d058a466"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
